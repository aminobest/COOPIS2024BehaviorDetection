{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ba1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "from scipy.stats import wilcoxon\n",
    "from constants import diffMappingToScore, questions, labelsToElements\n",
    "from utils import fixationProportionThresholdAnalysis, phaseDetection, dwellRegressionOnRelevantElements, periodCalculation, scanPathPrecision, averageFixationDuration, averageSaccadeAmplitudeForPhases, addQuestionInfo\n",
    "\n",
    "from functools import reduce\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from adjustText import adjust_text\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.signal import argrelextrema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8a4f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_csv(\"data/eventsDataWithAois.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['participant'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6739e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enrich questions with relevant elements\n",
    "questions = [ {**question,**{'Relevant elements labels': re.findall('\"(.+?)\"', question[\"question\"])}}  for question in questions ]\n",
    "\n",
    "for question in questions:\n",
    "    for idx, label in enumerate(question[\"Relevant elements labels\"]):\n",
    "        if re.compile(\"\\[(.+?)\\]\").match(label):\n",
    "            question[\"Relevant elements labels\"][idx-1] = f'{question[\"Relevant elements labels\"][idx-1]} {label}'\n",
    "            question[\"Relevant elements labels\"].remove(label)\n",
    "            \n",
    "questions = [ {**question,**{'Relevant elements count': len(question[\"Relevant elements labels\"])}}  for question in questions ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get activities labels\n",
    "questions = [ {**question,**{'Relevant elements names':  [ labelsToElements[activity] for  activity in question[\"Relevant elements labels\"] ]   }}  for question in questions ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#\n",
    "# Phase detection\n",
    "#\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774358b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop na\n",
    "fixationData = data.loc[(~data['FixID'].isna()) & (~data['currentQuestion'].isna())].copy(deep=True)\n",
    "#add question info\n",
    "fixationData = addQuestionInfo(fixationData,questions)\n",
    "\n",
    "\"\"\"Q13 (local) and Q25 (global) need to be removed for SP11\"\"\"\n",
    "fixationData = fixationData.drop(fixationData[(fixationData['participant'] == 'SP11-no') & (fixationData['Type3'] == 'Exclusiveness')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491983b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixationData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detect phases\n",
    "phDectFix = phaseDetection(fixationData,questions)\n",
    "\n",
    "#add Timestamp_formatted column\n",
    "phDectFix[\"timestamp_formatted\"] = pd.to_datetime(phDectFix['Fixation Start'], unit='ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129127e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial number of trials (considering control-flow questions only) (considering that \"\"\"Q13 (local) and Q25 (global) need to be removed for SP11\"\"\")\n",
    "initial_trials = fixationData.loc[fixationData[\"Type2\"]==\"Control-flow\"].groupby(['participant', 'currentQuestion']).size().reset_index(name='count')\n",
    "num_initial_trials = len(initial_trials)\n",
    "print(\"Initial number of trials:\",num_initial_trials)\n",
    "\n",
    "#Number of trials where participants identified all relevant activities (i.e., the cut is possible to make):\n",
    "trials_with_phaseCut = phDectFix.loc[(phDectFix[\"Type2\"]==\"Control-flow\") & (phDectFix[\"Phase\"]!=\"N/A\")].groupby(['participant', 'currentQuestion']).size().reset_index(name='count')\n",
    "num_trials_with_phaseCut = len(trials_with_phaseCut)\n",
    "print(\"trials where participants identified all relevant activities:\",num_trials_with_phaseCut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb83ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "phDectFix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "#\n",
    "# ML\n",
    "#\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7771cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(freq, phDectFix, data):\n",
    "    \n",
    "    print(\"calculate_metrics()\")\n",
    "    \n",
    "    #######################\n",
    "    #\n",
    "    # Average fixation duration\n",
    "    #\n",
    "    #######################\n",
    "    #filter out those with N/A\n",
    "    print(\"Average fixation duration\")\n",
    "    avFDPT = averageFixationDuration(phDectFix,['Type1','Type2','Type3','Phase',pd.Grouper(key='timestamp_formatted', freq=freq)])\n",
    "    avFDPT = avFDPT.loc[avFDPT[\"Phase\"]!=\"N/A\"].copy(deep=True)\n",
    "    #Keep only control-flow\n",
    "    avFDPT = avFDPT.loc[avFDPT[\"Type2\"]==\"Control-flow\"].copy(deep=True)\n",
    "    # add incremental ID to each group\n",
    "    avFDPT['phaseTimeIntervalCounter'] = avFDPT.groupby(['currentQuestion','participant','Type1','Type2','Type3','Phase']).cumcount()\n",
    "    #sorting \n",
    "    avFDPT = avFDPT.sort_values(by=['participant','currentQuestion','timestamp'])\n",
    "    ####################\n",
    "    #\n",
    "    # Average Saccade amplitude\n",
    "    #\n",
    "    ####################\n",
    "    print(\"Average Saccade amplitude\")    \n",
    "    #filter saccadeData\n",
    "    filtered_data = data.loc[(~data['SacID'].isna()) & (~data['currentQuestion'].isna())].copy(deep=True)\n",
    "    #filter out those with N/A\n",
    "    phases = phDectFix.loc[phDectFix[\"Phase\"]!=\"N/A\"].copy(deep=True)\n",
    "    #Keep only control-flow\n",
    "    phases = phases.loc[phases[\"Type2\"]==\"Control-flow\"].copy(deep=True)\n",
    "    #calculate avSacAmplitude\n",
    "    avSacAmplitude = averageSaccadeAmplitudeForPhases(phases,filtered_data,['currentQuestion','participant','Type1','Type2','Type3','Phase',pd.Grouper(key='timestamp_formatted', freq=freq)])\n",
    "    # add incremental ID to each group\n",
    "    avSacAmplitude['phaseTimeIntervalCounter'] = avSacAmplitude.groupby(['currentQuestion','participant','Type1','Type2','Type3','Phase']).cumcount()\n",
    "    #sorting (extra)\n",
    "    avSacAmplitude = avSacAmplitude.sort_values(by=['participant','currentQuestion','timestamp'])\n",
    "    ####################\n",
    "    #\n",
    "    # Scan-path precision\n",
    "    #\n",
    "    ####################\n",
    "    print(\"Scan-path precision\")\n",
    "    scanPathPrecisionData = scanPathPrecision(phDectFix,['Type1','Type2','Type3','Phase',pd.Grouper(key='timestamp_formatted', freq=freq)])\n",
    "    #filter out those with N/A\n",
    "    scanPathPrecisionData = scanPathPrecisionData.loc[scanPathPrecisionData[\"Phase\"]!=\"N/A\"].copy(deep=True)\n",
    "    #Keep only control-flow\n",
    "    scanPathPrecisionData = scanPathPrecisionData.loc[scanPathPrecisionData[\"Type2\"]==\"Control-flow\"].copy(deep=True)\n",
    "    # add incremental ID to each group\n",
    "    scanPathPrecisionData['phaseTimeIntervalCounter'] = scanPathPrecisionData.groupby(['currentQuestion','participant','Type1','Type2','Type3','Phase']).cumcount()\n",
    "    #sorting (extra)\n",
    "    scanPathPrecisionData = scanPathPrecisionData.sort_values(by=['participant','currentQuestion','timestamp'])\n",
    "    #######################\n",
    "    #\n",
    "    # Fixation threshold proportion analysis\n",
    "    #\n",
    "    #######################\n",
    "    print(\"Fixation threshold proportion analysis\")\n",
    "    fxThresholdsData = fixationProportionThresholdAnalysis(phDectFix,['Type1','Type2','Type3','Phase',pd.Grouper(key='timestamp_formatted', freq=freq)])\n",
    "    #filter out those with N/A\n",
    "    fxThresholdsData = fxThresholdsData.loc[fxThresholdsData[\"Phase\"]!=\"N/A\"].copy(deep=True)\n",
    "    #Keep only control-flow\n",
    "    fxThresholdsData = fxThresholdsData.loc[fxThresholdsData[\"Type2\"]==\"Control-flow\"].copy(deep=True)\n",
    "    # add incremental ID to each group\n",
    "    fxThresholdsData['phaseTimeIntervalCounter'] = fxThresholdsData.groupby(['currentQuestion','participant','Type1','Type2','Type3','Phase']).cumcount()\n",
    "    #sorting (extra)\n",
    "    fxThresholdsData = fxThresholdsData.sort_values(by=['participant','currentQuestion','timestamp'])\n",
    "    #######################\n",
    "    #\n",
    "    # All measures in one dataframe\n",
    "    #\n",
    "    ######################\n",
    "    #merge all dataframes (computed previously)\n",
    "    dfs = [avFDPT,\n",
    "           avSacAmplitude,\n",
    "           scanPathPrecisionData,fxThresholdsData]\n",
    "    all_measures = reduce(lambda left,right: pd.merge(left,right,on=['participant', 'currentQuestion', 'Type1', 'Type2', 'Type3', 'Phase','phaseTimeIntervalCounter','timestamp'], how='inner'), dfs)\n",
    "    all_measures.columns\n",
    "    \n",
    "    #drop N/A\n",
    "    print(len(all_measures))\n",
    "    all_measures = all_measures.dropna()\n",
    "    print(\"after removing nans\",len(all_measures))\n",
    "    \n",
    "    \n",
    "    # Reset the index of the filtered DataFrame\n",
    "    all_measures.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #sorting \n",
    "    all_measures = all_measures.sort_values(by=['participant','currentQuestion','timestamp'])\n",
    "\n",
    "\n",
    "    return all_measures\n",
    "\n",
    "\n",
    "def run_cross_validation(all_measures, freq, cross_validation_methods, scoring_metrics):\n",
    "    \n",
    "    print(\"run_cross_validation()\")\n",
    "    \n",
    "    benchmarks = []\n",
    "\n",
    "    #########################\n",
    "    #\n",
    "    # ML Prediction and validation\n",
    "    #\n",
    "    #########################\n",
    "\n",
    "    #features and labels\n",
    "    features = ['Average_Fixation_Duration', \n",
    "                      'avSaccadeAmplitude',\n",
    "    'scan_path_precision', 'shortFixationsProp', \n",
    "                    \n",
    "    'longFixationsProp']\n",
    "    X = all_measures[features]\n",
    "    y = all_measures['Phase']\n",
    "    \n",
    "\n",
    "    #Define the Random Forest classifier with the given parameters\n",
    "    rf_clf =  RandomForestClassifier(max_depth=5, n_estimators=300, class_weight='balanced')\n",
    "\n",
    "\n",
    "    # Iterate over the cross-validation methods\n",
    "    for method in cross_validation_methods:\n",
    "        \n",
    "        print(\"----\", method)\n",
    "\n",
    "        # Get training and testing folds\n",
    "        cross_validation_folds = constructcvs(all_measures, method, all_measures['participant'].unique(), all_measures['currentQuestion'].unique())\n",
    "\n",
    "        # Calculate performance metrics using cross_validate and the random forest classifier\n",
    "        cv_results = cross_validate(rf_clf, X, y, cv=cross_validation_folds, scoring=scoring_metrics)\n",
    "\n",
    "        precision_value = cv_results['test_precision_weighted'].mean()\n",
    "        recall_value = cv_results['test_recall_weighted'].mean()\n",
    "        f1_score_value = cv_results['test_f1_weighted'].mean()\n",
    "        \n",
    "        # Fit the model to the data and get the feature importance\n",
    "        rf_clf.fit(X, y)\n",
    "        feature_importance = rf_clf.feature_importances_\n",
    "        \n",
    "        # Create a dictionary mapping feature names to their importances\n",
    "        feature_importance_dict = dict(zip(features, feature_importance))\n",
    "        \n",
    "        # Benchmarking\n",
    "        benchmark = {\n",
    "            \"maxWindowSize\": freq,\n",
    "            \"Method\": method + \"-out\",\n",
    "            \"Classifier\": \"Random Forest\",\n",
    "            \"Precision\": precision_value,\n",
    "            \"Recall\": recall_value,\n",
    "            \"F1 Score\": f1_score_value,\n",
    "        }\n",
    "        \n",
    "        # Add feature importance to benchmark\n",
    "        for feature_name, importance in feature_importance_dict.items():\n",
    "            benchmark[f'feature_importance_{feature_name}'] = importance\n",
    "\n",
    "        print(benchmark)\n",
    "\n",
    "        benchmarks.append(benchmark)\n",
    "\n",
    "    return benchmarks\n",
    "\n",
    "\n",
    " #extract a set of train and test folds from a given dataset\n",
    "def constructcvs(dataset,evalMethod,participantsList,taskList):\n",
    "\n",
    "    cv = None \n",
    "\n",
    "    if evalMethod=='participant':\n",
    "        print(\"cross validation method:\"+evalMethod)\n",
    "        cv = list()\n",
    "        for participantToTtest in participantsList:\n",
    "\n",
    "            train =  dataset.index[dataset['participant']!=participantToTtest]\n",
    "            test =  dataset.index[dataset['participant']==participantToTtest]\n",
    "            if len(test)>0:\n",
    "                cv.append((train, test))\n",
    "     \n",
    "    if evalMethod=='task':\n",
    "        print(\"cross validation method:\"+evalMethod)\n",
    "        cv = list()\n",
    "        for taskToTtest in taskList:\n",
    "\n",
    "            train =  dataset.index[dataset['currentQuestion']!=taskToTtest]\n",
    "            test = dataset.index[dataset['currentQuestion']==taskToTtest]\n",
    "            if len(test)>0:\n",
    "                cv.append((train, test))\n",
    "\n",
    "\n",
    "    if evalMethod=='participant-task':\n",
    "        print(\"cross validation method:\"+evalMethod)\n",
    "        cv = list()\n",
    "\n",
    "        for participantToTtest in participantsList:\n",
    "            for taskToTtest in taskList:\n",
    "\n",
    "                train =  dataset.index[(dataset['currentQuestion']!=taskToTtest) &   (dataset['participant']!=participantToTtest)   ]\n",
    "                test = dataset.index[(dataset['currentQuestion']==taskToTtest) &   (dataset['participant']==participantToTtest)     ]\n",
    "                if len(test)>0:\n",
    "                    cv.append((train, test))\n",
    "                    \n",
    "    return cv    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d6b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation methods\n",
    "cross_validation_methods = [\n",
    "                            'participant',\n",
    "                            'task',\n",
    "                            'participant-task'\n",
    "                           ]\n",
    "\n",
    "# Define the scoring metrics\n",
    "scoring_metrics = ['precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab2ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb088dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_benchmarks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac7acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for freqInt in [5,10,15,20,25,30,35,40,45,50,55,60]:\n",
    "    \n",
    "    freq = str(freqInt)+'s'\n",
    "    \n",
    "    print(\"--------\",freq)\n",
    "    \n",
    "    all_measures = calculate_metrics(freq, phDectFix, data)\n",
    "    benchmarks = run_cross_validation(all_measures, freq, cross_validation_methods, scoring_metrics,)\n",
    "    all_benchmarks.extend(benchmarks)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a5b6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31202ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert maxWindowSize to integers for easier sorting\n",
    "df = pd.DataFrame(all_benchmarks)\n",
    "\n",
    "df['maxWindowSize'] = df['maxWindowSize'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# Group data by Method\n",
    "grouped = df.groupby('Method')\n",
    "\n",
    "# Function to annotate data points\n",
    "def annotate_points(x, y, ax, label, color, fontsize=8):\n",
    "    texts = []\n",
    "    for i, txt in enumerate(y):\n",
    "        texts.append(ax.annotate(f\"{label}{txt:.2f}\", (x[i], y[i]), textcoords=\"offset points\", \n",
    "                                 xytext=(0, 5), ha='center', fontsize=fontsize, color=color))\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Iterate over the grouped data and plot the performance metrics\n",
    "for method, group in grouped:\n",
    "    group = group.reset_index(drop=True)  # Reset the index of the group DataFrame\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))  # Increase the figure size\n",
    "    plt.title(f\"Performance Metrics for {method}\")\n",
    "\n",
    "    # Plot lines and store their colors\n",
    "\n",
    "    \n",
    "    prec_line, = ax.plot(group['maxWindowSize'], group['Precision'], label='Precision', marker='o')\n",
    "    prec_color = prec_line.get_color()\n",
    "    \n",
    "    f1_line, = ax.plot(group['maxWindowSize'], group['F1 Score'], label='F1 Score', marker='o')\n",
    "    f1_color = f1_line.get_color()\n",
    "    \n",
    "    acc_line, = ax.plot(group['maxWindowSize'], group['Recall'], label='Recall', marker='o')\n",
    "    acc_color = acc_line.get_color()\n",
    "\n",
    "    plt.xlabel('Window Size')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Set x-ticks for every 5 seconds if needed\n",
    "    max_window = group['maxWindowSize'].max()\n",
    "    plt.xticks(range(min(group['maxWindowSize']), max_window + 5, 5))  # Adjusting the range and step for x-ticks\n",
    "\n",
    "\n",
    "    # Annotate data points with corresponding line colors\n",
    "    texts = []\n",
    "    texts.extend(annotate_points(group['maxWindowSize'], group['Recall'], ax, '', acc_color))\n",
    "    texts.extend(annotate_points(group['maxWindowSize'], group['Precision'], ax, '', prec_color))\n",
    "    texts.extend(annotate_points(group['maxWindowSize'], group['F1 Score'], ax, '', f1_color))\n",
    "\n",
    "    # Adjust text positions to avoid overlap\n",
    "    adjust_text(texts)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bd7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "features = ['Average_Fixation_Duration', \n",
    "                      'avSaccadeAmplitude',\n",
    "    'scan_path_precision', 'shortFixationsProp', \n",
    "    'longFixationsProp']\n",
    "\n",
    "# Define color palette\n",
    "color_palette = sns.color_palette(\"dark\", len(df.columns[df.columns.str.startswith('feature_importance_')]))\n",
    "\n",
    "\n",
    "# Getting all the feature importance columns\n",
    "feature_cols = df.columns[df.columns.str.startswith('feature_importance_')]\n",
    "\n",
    "# For each feature\n",
    "for idx, feature in enumerate(feature_cols):\n",
    "    # Group by maxWindowSize, calculate mean and reset index\n",
    "    df_grouped = df.groupby('maxWindowSize')[feature].mean().reset_index()\n",
    "    \n",
    "    # Plotting\n",
    "    plt.plot(df_grouped['maxWindowSize'], df_grouped[feature], \n",
    "             color=color_palette[idx], label=f'{feature.replace(\"feature_importance_\", \"\")}')\n",
    "\n",
    "    # Find local maxima\n",
    "    y_values = df_grouped[feature].values\n",
    "    local_maxima = argrelextrema(y_values, np.greater)[0]\n",
    "\n",
    "    # Adding annotations at local peaks\n",
    "    for max_index in local_maxima:\n",
    "        x_value = df_grouped['maxWindowSize'][max_index]\n",
    "        y_value = y_values[max_index]\n",
    "        plt.text(x_value, y_value, f'{y_value:.2f}', color=color_palette[idx], fontsize=8, ha='center')\n",
    "\n",
    "# Set x-ticks for every 5 seconds if needed\n",
    "max_window = group['maxWindowSize'].max()\n",
    "plt.xticks(range(min(group['maxWindowSize']), max_window + 5, 5))  # Adjusting the range and step for x-ticks\n",
    "\n",
    "plt.title('Features importance at different window lengths')\n",
    "plt.xlabel('WindowSize')\n",
    "plt.ylabel('Average Feature Importance')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cfc40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in feature_cols:\n",
    "    print(feature, df[feature].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5a906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
